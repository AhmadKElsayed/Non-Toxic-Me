{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 0. Importing Libraries","metadata":{}},{"cell_type":"code","source":"#Main\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#Utilities\nimport random\nimport re\nimport pickle\nfrom tqdm.auto import tqdm\n\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, precision_recall_curve\n\n#torch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#Hugging Face\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers import set_seed\nfrom transformers import get_linear_schedule_with_warmup\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Random States:","metadata":{}},{"cell_type":"code","source":"# Python and Numpy\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Save states (optional, for later restoration)\npython_state = random.getstate()\nnumpy_state = np.random.get_state()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch \ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # For multi-GPU\ntorch.backends.cudnn.deterministic = True  # Slower but reproducible\ntorch.backends.cudnn.benchmark = False\n\n# Save RNG states\ntorch_rng_state = torch.get_rng_state()\ncuda_rng_state = torch.cuda.get_rng_state() if torch.cuda.is_available() else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hugging Face\nset_seed(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving all random states\n\nrandom_states = {\n    \"python\": random.getstate(),\n    \"numpy\": np.random.get_state(),\n    \"torch_cpu\": torch.get_rng_state(),\n    \"torch_cuda\": torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n    \"sklearn_seed\": seed  # For train_test_split\n}\n\n# Save to file\nwith open(\"random_states.pkl\", \"wb\") as f:\n    pickle.dump(random_states, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Load and Preprocess the Data:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ndf_test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to remove punctuation using regular expressions\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_punctuation)\n\n# Define a function to remove special characters using regular expressions\ndef remove_special_characters(text):\n    # Define a regular expression pattern to match special characters\n    pattern = r'[^a-zA-Z0-9\\s]'  # This pattern matches any character that is not a letter, digit, or whitespace\n    return re.sub(pattern, '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_special_characters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"comments = df_train[\"comment_text\"].tolist()\nlabels = df_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\ntrain_comments, val_comments, train_labels, val_labels = train_test_split(\n    comments, labels, test_size=0.001, #0.001 3shan hdtr a edit el notebook kolha\n    random_state=seed\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Tokenization:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Changed to BERT\n\ndef tokenize(texts):\n    return tokenizer(\n        texts, \n        padding=True, \n        truncation=True, \n        max_length=128, \n        return_tensors=\"pt\"\n    )\n\n# Tokenize data (works exactly the same way)\ntrain_encodings = tokenize(train_comments)\nval_encodings = tokenize(val_comments)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. PyTorch Dataset:","metadata":{}},{"cell_type":"code","source":"class ToxicDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": torch.FloatTensor(self.labels[idx])\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = ToxicDataset(train_encodings, train_labels)\nval_dataset = ToxicDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. BERT Model:","metadata":{}},{"cell_type":"markdown","source":"### a. Load Pre-trained Roberta Model:","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#Ems7ha lw 3awz trg3 el adeem\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1 - pt) ** self.gamma * bce_loss).mean()\n        return loss\n\n# Set Focal Loss as the model's loss function\nmodel.loss_fct = FocalLoss(gamma=2).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Training Loop","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nepochs = 10\nlr = 2e-5\nwarmup_steps = 100\nmax_grad_norm = 1.0\n\n# Training Setup\noptimizer = AdamW(model.parameters(), lr=lr)\n\n# Learning rate schedule\ndef lr_lambda(current_step):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    return 1.0\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n# Tracking\nhistory = []\n\nfor epoch in range(epochs):\n    # --- Training Phase ---\n    model.train()\n    train_loss = 0\n    \n    # Progress bar for epoch\n    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False) as progress_bar:\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].to(model.device)\n\n            outputs = model(**inputs, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n            # Update tqdm progress bar with current batch loss\n            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n    \n    avg_train_loss = train_loss / len(train_loader)\n\n    # --- Progress Tracking ---\n    history.append({\n        'epoch': epoch + 1,\n        'train_loss': avg_train_loss\n    })\n\n    print(f\"\\nEpoch {epoch + 1} Results:\")\n    print(f\"Train Loss: {avg_train_loss:.4f}\")\n\n# Save final model after training\ntorch.save(model.state_dict(), \"final_model.pt\")\nprint(\"Saved final model weights.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Uncomment lma tst3ml train test split set\n'''\n# Hyperparameters\nepochs = 10 # Increased to allow early stopping to work\nlr = 2e-5\nwarmup_steps = 100\nmax_grad_norm = 1.0\npatience = 3  # Number of epochs to wait before stopping\n\n# Training Setup\noptimizer = AdamW(model.parameters(), lr=lr)\n\n# Learning rate schedule\ndef lr_lambda(current_step):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    return 1.0\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n# Tracking\nbest_metrics = {\n    'val_loss': float('inf'),\n    'weights': None,\n    'epoch': -1\n}\nhistory = []\nepochs_without_improvement = 0  # Early stopping counter\n\nfor epoch in range(epochs):\n    # --- Training Phase ---\n    model.train()\n    train_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(model.device)\n        \n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n    \n    avg_train_loss = train_loss / len(train_loader)\n\n    # --- Validation Phase ---\n    avg_val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].to(model.device)\n            \n            outputs = model(**inputs, labels=labels)\n            avg_val_loss += outputs.loss.item()\n    \n    avg_val_loss /= len(val_loader)\n\n    # --- Early Stopping Check ---\n    if avg_val_loss < best_metrics['val_loss']:\n        best_metrics.update({\n            'val_loss': avg_val_loss,\n            'weights': model.state_dict().copy(),\n            'epoch': epoch + 1\n        })\n        torch.save(best_metrics['weights'], \"best_model.pt\")\n        epochs_without_improvement = 0  # Reset counter\n        print(f\"↳ New best model saved! (Loss: {avg_val_loss:.4f})\")\n    else:\n        epochs_without_improvement += 1\n        print(f\"↳ No improvement ({epochs_without_improvement}/{patience})\")\n        \n        if epochs_without_improvement >= patience:\n            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}!\")\n            print(f\"Best model was from epoch {best_metrics['epoch']} with val_loss {best_metrics['val_loss']:.4f}\")\n            break\n\n    # --- Progress Tracking ---\n    history.append({\n        'epoch': epoch + 1,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'early_stop_counter': epochs_without_improvement\n    })\n    \n    print(f\"\\nEpoch {epoch + 1} Results:\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n# Final save if no best model was found\nif not os.path.exists(\"best_model.pt\"):\n    torch.save(model.state_dict(), \"final_model.pt\")\n    print(\"Saved final model weights (no improvement during training)\")\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T21:30:18.637930Z","iopub.execute_input":"2025-04-04T21:30:18.638135Z","iopub.status.idle":"2025-04-04T21:30:18.645466Z","shell.execute_reply.started":"2025-04-04T21:30:18.638116Z","shell.execute_reply":"2025-04-04T21:30:18.644512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Model Evaluation:","metadata":{}},{"cell_type":"markdown","source":"### a. Calculating Metrics","metadata":{}},{"cell_type":"code","source":"model.eval()  # Set model to evaluation mode\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in val_loader:  # Use validation DataLoader\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].cpu().numpy()\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        val_preds.extend(probs)\n        val_true.extend(labels)\n\nval_preds = np.array(val_preds)\nval_true = np.array(val_true)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lma tst3ml train test split bgd\n'''\n# Classification Report (threshold to be controlled)\nprint(classification_report(\n    val_true, \n    val_preds > 0.5,  # Binary predictions\n    target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n))\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Visualizing:","metadata":{}},{"cell_type":"code","source":"# Uncomment lma tst3ml train test split bgd\n'''\nplt.figure(figsize=(10, 6))\nfor i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    precision, recall, _ = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    plt.plot(recall, precision, label=label)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.legend()\nplt.title(\"Precision-Recall Curves\")\nplt.show()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Finding Optimal Threshold:","metadata":{}},{"cell_type":"code","source":"#Uncomment lma tst3ml train test split bgd\n'''\noptimal_thresholds = []\nfor i in range(6):\n    precision, recall, thresholds = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n    optimal_thresholds.append(thresholds[np.argmax(f1_scores)])\nprint(f\"Optimal Thresholds: {optimal_thresholds}\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_summary())  # Place after each epoch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Making Predictions:","metadata":{}},{"cell_type":"code","source":"# 1. Tokenize test data\ntest_encodings = tokenizer(\n    df_test[\"comment_text\"].tolist(),\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Define Dataset class\nclass TestDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n        }\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Create Dataset and Loader\ntest_dataset = TestDataset(test_encodings)\nloader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Run inference\nmodel.eval()\nall_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(loader, desc=\"Processing\"):\n        inputs = {k: v.to(model.device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        all_probs.extend(probs)\n\n        # Memory cleanup\n        del inputs, outputs, batch\n        torch.cuda.empty_cache()\n\n# 5. Final predictions array\nprobs = np.vstack(all_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport numpy as np\n\ndef find_optimal_thresholds(val_true, val_preds, toxicity_classes):\n    \"\"\"\n    Calculate optimal thresholds maximizing F1 for each class\n    \n    Args:\n        val_true: Array of true labels (n_samples × n_classes)\n        val_preds: Array of predicted probabilities (n_samples × n_classes)\n        toxicity_classes: List of class names\n        \n    Returns:\n        Dictionary of {class_name: optimal_threshold}\n    \"\"\"\n    optimal_thresholds = {}\n    \n    for i, class_name in enumerate(toxicity_classes):\n        # Get precision-recall curve for this class\n        precision, recall, thresholds = precision_recall_curve(\n            val_true[:, i], \n            val_preds[:, i]\n        )\n        \n        # Calculating F1 scores\n        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n        \n        # Finding threshold with max F1\n        optimal_idx = np.argmax(f1_scores)\n        optimal_threshold = thresholds[optimal_idx]\n        \n        # Store optimal threshold in dictionary\n        optimal_thresholds[class_name] = optimal_threshold\n    \n    return optimal_thresholds\n\n# Calling the function\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\noptimal_thresholds = find_optimal_thresholds(val_true, val_preds, toxicity_classes)\n\n# Resulting dictionary\nprint(optimal_thresholds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# When using train test split est3ml dh w comment elly b3doh\n'''\n# Optimal thresholds (tune these on your validation set)\nclass_thresholds = optimal_thresholds\n\n# Add probability columns (unchanged)\nfor i, col in enumerate(toxicity_classes):\n    df_test[f\"{col}_prob\"] = probs[:, i]\n    \n# Apply custom thresholds for binary predictions\nfor col in toxicity_classes:\n    df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)\n\n# Check class distribution after thresholding\nprint(\"\\nPredicted Positive Rates:\")\nfor col in toxicity_classes:\n    rate = df_test[f\"{col}_pred\"].mean()\n    print(f\"{col}: {rate:.2%} (threshold={class_thresholds[col]})\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add probability columns (unchanged)\nfor i, col in enumerate(toxicity_classes):\n    df_test[f\"{col}_prob\"] = probs[:, i]\n    \n# Apply custom thresholds for binary predictions\nfor col in toxicity_classes:\n    #df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)   #When usibng train test split\n    df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > 0.5).astype(int)\n\n# Check class distribution after thresholding\nprint(\"\\nPredicted Positive Rates:\")\nfor col in toxicity_classes:\n    rate = df_test[f\"{col}\"].mean()\n    print(f\"{col}: {rate:.2%} (threshold={0.5})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs[:, i]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = df_test.drop(['comment_text'], axis = 1)\nfor col in toxicity_classes:\n    submission.drop([f\"{col}_prob\"], inplace = True, axis = 1)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('bert_model_weights')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 99. Try","metadata":{}},{"cell_type":"markdown","source":"#### Single Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"# Load your trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\nmodel.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Move model to device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n\n# Your input sentence\nsentence = \"SLUT\"\n\n# Tokenize and predict\ninputs = tokenizer(sentence, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # Get probabilities\n\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n\n# results\nprint(sentence)\n\nprint(\"Predicted probabilities:\")\nfor cls, prob in zip(toxicity_classes, probs):\n    print(f\"{cls}: {prob:.4f}\")\n\n# binary predictions\nbinary_preds = (probs > 0.5).astype(int)\nprint(\"\\nBinary predictions (threshold=0.5):\")\nfor cls, pred in zip(toxicity_classes, binary_preds):\n    print(f\"{cls}: {'✅' if pred else '❌'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Multi-Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"sentences = [\n    \"You're stupid!\",\n    \"Thanks for your help\",\n    \"Go back to your country\"\n]\n\n# Tokenize batch\ninputs = tokenizer(sentences, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\n# Predict\nwith torch.no_grad():\n    outputs = model(**inputs)\n    all_probs = torch.sigmoid(outputs.logits).cpu().numpy()\n\n# Display results\nfor i, sentence in enumerate(sentences):\n    print(f\"\\nSentence: '{sentence}'\")\n    for cls, prob in zip(toxicity_classes, all_probs[i]):\n        print(f\"{cls}: {prob:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Threat Problem:","metadata":{}},{"cell_type":"markdown","source":"### a. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes=6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.ce_loss = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, inputs, targets):\n        loss = self.ce_loss(inputs, targets)\n        p_t = torch.exp(-loss)\n        focal_loss = self.alpha * (1 - p_t) ** self.gamma * loss\n        return focal_loss.mean()\n\nclass CustomBERTForSequenceClassificationWithFocalLoss(BertForSequenceClassification):\n    def __init__(self, config, focal_loss_alpha=0.25, focal_loss_gamma=2):\n        super().__init__(config)\n        self.focal_loss = FocalLoss(alpha=focal_loss_alpha, gamma=focal_loss_gamma)\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        # Call the parent model's forward method\n        outputs = super().forward(input_ids=input_ids, \n                                  attention_mask=attention_mask, \n                                  token_type_ids=token_type_ids, \n                                  **kwargs)\n        logits = outputs.logits\n\n        # Compute loss if labels are provided\n        if labels is not None:\n            loss = self.focal_loss(logits, labels)\n            return (loss, outputs)\n        else:\n            return outputs\n\n# Example usage:\nmodel = CustomBERTForSequenceClassificationWithFocalLoss.from_pretrained(\"bert-base-uncased\", num_labels=6).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Class Weights","metadata":{}},{"cell_type":"code","source":"'''\n# Calculate class weights (inverse of class frequencies)\nclass_counts = np.array([sum(train_labels[:, i]) for i in range(6)])  # Count per class\nclass_weights = torch.tensor(\n    (1.0 / (class_counts + 1e-6)) * (len(train_labels)/6),  # Normalize\n    dtype=torch.float32,\n    device=device\n)\n\n# Modify your model initialization\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n)\nmodel.loss_fct = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1-pt)**self.gamma * bce_loss).mean()\n        return loss\n\nmodel.loss_fct = FocalLoss(gamma=2).to(device)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### d. Data Loader","metadata":{}},{"cell_type":"code","source":"'''\nfrom torch.utils.data import WeightedRandomSampler\n\n# Calculate sample weights (higher for threat-containing samples)\nsample_weights = torch.where(\n    train_labels[:, 3] == 1,  # Threat is index 3\n    torch.tensor(50.0),       # 50x higher sampling for threats\n    torch.tensor(1.0)\n)\n\nsampler = WeightedRandomSampler(\n    sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Modify your DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    sampler=sampler,  # Replaces shuffle=True\n    num_workers=4\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}