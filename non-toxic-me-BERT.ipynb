{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:10.933748Z","iopub.execute_input":"2025-04-04T18:15:10.934116Z","iopub.status.idle":"2025-04-04T18:15:10.942216Z","shell.execute_reply.started":"2025-04-04T18:15:10.934085Z","shell.execute_reply":"2025-04-04T18:15:10.941392Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 0. Importing Libraries","metadata":{}},{"cell_type":"code","source":"#Main\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#Utilities\nimport random\nimport re\nimport pickle\nfrom tqdm.auto import tqdm\n\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, precision_recall_curve\n\n#torch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#Hugging Face\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers import set_seed\nfrom transformers import get_linear_schedule_with_warmup\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:10.983182Z","iopub.execute_input":"2025-04-04T18:15:10.983512Z","iopub.status.idle":"2025-04-04T18:15:20.364432Z","shell.execute_reply.started":"2025-04-04T18:15:10.983484Z","shell.execute_reply":"2025-04-04T18:15:20.363573Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#### Random States:","metadata":{}},{"cell_type":"code","source":"# Python and Numpy\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Save states (optional, for later restoration)\npython_state = random.getstate()\nnumpy_state = np.random.get_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:20.365559Z","iopub.execute_input":"2025-04-04T18:15:20.366049Z","iopub.status.idle":"2025-04-04T18:15:20.372129Z","shell.execute_reply.started":"2025-04-04T18:15:20.366023Z","shell.execute_reply":"2025-04-04T18:15:20.369737Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# torch \ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # For multi-GPU\ntorch.backends.cudnn.deterministic = True  # Slower but reproducible\ntorch.backends.cudnn.benchmark = False\n\n# Save RNG states\ntorch_rng_state = torch.get_rng_state()\ncuda_rng_state = torch.cuda.get_rng_state() if torch.cuda.is_available() else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:20.374437Z","iopub.execute_input":"2025-04-04T18:15:20.374946Z","iopub.status.idle":"2025-04-04T18:15:20.505356Z","shell.execute_reply.started":"2025-04-04T18:15:20.374903Z","shell.execute_reply":"2025-04-04T18:15:20.504594Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Hugging Face\nset_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:20.506868Z","iopub.execute_input":"2025-04-04T18:15:20.507235Z","iopub.status.idle":"2025-04-04T18:15:33.307160Z","shell.execute_reply.started":"2025-04-04T18:15:20.507200Z","shell.execute_reply":"2025-04-04T18:15:33.306426Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Saving all random states\n\nrandom_states = {\n    \"python\": random.getstate(),\n    \"numpy\": np.random.get_state(),\n    \"torch_cpu\": torch.get_rng_state(),\n    \"torch_cuda\": torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n    \"sklearn_seed\": seed  # For train_test_split\n}\n\n# Save to file\nwith open(\"random_states.pkl\", \"wb\") as f:\n    pickle.dump(random_states, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:33.307887Z","iopub.execute_input":"2025-04-04T18:15:33.308349Z","iopub.status.idle":"2025-04-04T18:15:33.316681Z","shell.execute_reply.started":"2025-04-04T18:15:33.308327Z","shell.execute_reply":"2025-04-04T18:15:33.315721Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 1. Load and Preprocess the Data:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ndf_test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:33.317521Z","iopub.execute_input":"2025-04-04T18:15:33.317935Z","iopub.status.idle":"2025-04-04T18:15:37.090990Z","shell.execute_reply.started":"2025-04-04T18:15:33.317893Z","shell.execute_reply":"2025-04-04T18:15:37.090126Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:37.091956Z","iopub.execute_input":"2025-04-04T18:15:37.092297Z","iopub.status.idle":"2025-04-04T18:15:37.124815Z","shell.execute_reply.started":"2025-04-04T18:15:37.092263Z","shell.execute_reply":"2025-04-04T18:15:37.123698Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n0           0             0        0       0       0              0  \n1           0             0        0       0       0              0  \n2           0             0        0       0       0              0  \n3           0             0        0       0       0              0  \n4           0             0        0       0       0              0  \n...       ...           ...      ...     ...     ...            ...  \n159566      0             0        0       0       0              0  \n159567      0             0        0       0       0              0  \n159568      0             0        0       0       0              0  \n159569      0             0        0       0       0              0  \n159570      0             0        0       0       0              0  \n\n[159571 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>ffe987279560d7ff</td>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ffea4adeee384e90</td>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>ffee36eab5c267c9</td>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>fff125370e4aaaf3</td>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:37.127651Z","iopub.execute_input":"2025-04-04T18:15:37.127918Z","iopub.status.idle":"2025-04-04T18:15:37.138630Z","shell.execute_reply.started":"2025-04-04T18:15:37.127895Z","shell.execute_reply":"2025-04-04T18:15:37.137704Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                      id                                       comment_text\n0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3       00017563c3f7919a  :If you have a look back at the source, the in...\n4       00017695ad8997eb          I don't anonymously edit articles at all.\n...                  ...                                                ...\n153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n\n[153164 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>:If you have a look back at the source, the in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>I don't anonymously edit articles at all.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153159</th>\n      <td>fffcd0960ee309b5</td>\n      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n    </tr>\n    <tr>\n      <th>153160</th>\n      <td>fffd7a9a6eb32c16</td>\n      <td>== Throw from out field to home plate. == \\n\\n...</td>\n    </tr>\n    <tr>\n      <th>153161</th>\n      <td>fffda9e8d6fafa9e</td>\n      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n    </tr>\n    <tr>\n      <th>153162</th>\n      <td>fffe8f1340a79fc2</td>\n      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n    </tr>\n    <tr>\n      <th>153163</th>\n      <td>ffffce3fb183ee80</td>\n      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n    </tr>\n  </tbody>\n</table>\n<p>153164 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df_train.loc[df_train['toxic'] == 1, ['comment_text']].iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:37.140860Z","iopub.execute_input":"2025-04-04T18:15:37.141143Z","iopub.status.idle":"2025-04-04T18:15:37.169594Z","shell.execute_reply.started":"2025-04-04T18:15:37.141117Z","shell.execute_reply":"2025-04-04T18:15:37.168780Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"comment_text    COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\nName: 6, dtype: object"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Define a function to remove punctuation using regular expressions\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_punctuation)\n\n# Define a function to remove special characters using regular expressions\ndef remove_special_characters(text):\n    # Define a regular expression pattern to match special characters\n    pattern = r'[^a-zA-Z0-9\\s]'  # This pattern matches any character that is not a letter, digit, or whitespace\n    return re.sub(pattern, '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_special_characters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:37.170537Z","iopub.execute_input":"2025-04-04T18:15:37.170896Z","iopub.status.idle":"2025-04-04T18:15:39.690005Z","shell.execute_reply.started":"2025-04-04T18:15:37.170871Z","shell.execute_reply":"2025-04-04T18:15:39.689229Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"comments = df_train[\"comment_text\"].tolist()\nlabels = df_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n# Try all\ntrain_comments, val_comments, train_labels, val_labels = train_test_split(\n    comments, labels, test_size=0.001, random_state=seed\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:39.690872Z","iopub.execute_input":"2025-04-04T18:15:39.691112Z","iopub.status.idle":"2025-04-04T18:15:39.775187Z","shell.execute_reply.started":"2025-04-04T18:15:39.691091Z","shell.execute_reply":"2025-04-04T18:15:39.774112Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# 2. Tokenization:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Changed to BERT\n\ndef tokenize(texts):\n    return tokenizer(\n        texts, \n        padding=True, \n        truncation=True, \n        max_length=128, \n        return_tensors=\"pt\"\n    )\n\n# Tokenize data (works exactly the same way)\ntrain_encodings = tokenize(train_comments)\nval_encodings = tokenize(val_comments)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:15:39.776187Z","iopub.execute_input":"2025-04-04T18:15:39.776514Z","iopub.status.idle":"2025-04-04T18:16:19.521233Z","shell.execute_reply.started":"2025-04-04T18:15:39.776479Z","shell.execute_reply":"2025-04-04T18:16:19.520380Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dbe2b509d0044259f19761b05cc472f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d37977a239448f98184752e1819c96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb5d445acc5744c3a4769303cd338e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850962b9929548068996b6a3565ca48b"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# 3. PyTorch Dataset:","metadata":{}},{"cell_type":"code","source":"class ToxicDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": torch.FloatTensor(self.labels[idx])\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = ToxicDataset(train_encodings, train_labels)\nval_dataset = ToxicDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:16:19.522096Z","iopub.execute_input":"2025-04-04T18:16:19.522352Z","iopub.status.idle":"2025-04-04T18:16:19.528672Z","shell.execute_reply.started":"2025-04-04T18:16:19.522329Z","shell.execute_reply":"2025-04-04T18:16:19.527718Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 4. BERT Model:","metadata":{}},{"cell_type":"markdown","source":"### a. Load Pre-trained Roberta Model:","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#Ems7ha lw 3awz trg3 el adeem\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1 - pt) ** self.gamma * bce_loss).mean()\n        return loss\n\n# Set Focal Loss as the model's loss function\nmodel.loss_fct = FocalLoss(gamma=2).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:16:19.529601Z","iopub.execute_input":"2025-04-04T18:16:19.529891Z","iopub.status.idle":"2025-04-04T18:16:23.414668Z","shell.execute_reply.started":"2025-04-04T18:16:19.529868Z","shell.execute_reply":"2025-04-04T18:16:23.413872Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5e87f889ae4e28b36aedaf5901f2c7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### b. Training Loop","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nepochs = 1  # Increased to allow early stopping to work\nlr = 2e-5\nwarmup_steps = 100\nmax_grad_norm = 1.0\npatience = 3  # Number of epochs to wait before stopping\n\n# Training Setup\noptimizer = AdamW(model.parameters(), lr=lr)\n\n# Learning rate schedule\ndef lr_lambda(current_step):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    return 1.0\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n# Tracking\nbest_metrics = {\n    'val_loss': float('inf'),\n    'weights': None,\n    'epoch': -1\n}\nhistory = []\nepochs_without_improvement = 0  # Early stopping counter\n\nfor epoch in range(epochs):\n    # --- Training Phase ---\n    model.train()\n    train_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(model.device)\n        \n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n    \n    avg_train_loss = train_loss / len(train_loader)\n\n    # --- Validation Phase ---\n    avg_val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].to(model.device)\n            \n            outputs = model(**inputs, labels=labels)\n            avg_val_loss += outputs.loss.item()\n    \n    avg_val_loss /= len(val_loader)\n\n    # --- Early Stopping Check ---\n    if avg_val_loss < best_metrics['val_loss']:\n        best_metrics.update({\n            'val_loss': avg_val_loss,\n            'weights': model.state_dict().copy(),\n            'epoch': epoch + 1\n        })\n        torch.save(best_metrics['weights'], \"best_model.pt\")\n        epochs_without_improvement = 0  # Reset counter\n        print(f\"↳ New best model saved! (Loss: {avg_val_loss:.4f})\")\n    else:\n        epochs_without_improvement += 1\n        print(f\"↳ No improvement ({epochs_without_improvement}/{patience})\")\n        \n        if epochs_without_improvement >= patience:\n            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}!\")\n            print(f\"Best model was from epoch {best_metrics['epoch']} with val_loss {best_metrics['val_loss']:.4f}\")\n            break\n\n    # --- Progress Tracking ---\n    history.append({\n        'epoch': epoch + 1,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'early_stop_counter': epochs_without_improvement\n    })\n    \n    print(f\"\\nEpoch {epoch + 1} Results:\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n# Final save if no best model was found\nif not os.path.exists(\"best_model.pt\"):\n    torch.save(model.state_dict(), \"final_model.pt\")\n    print(\"Saved final model weights (no improvement during training)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:16:23.415709Z","iopub.execute_input":"2025-04-04T18:16:23.416000Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/1 [Train]:   0%|          | 0/9964 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3feb9ec9354edf98df9e430fd06dbc"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# 5. Model Evaluation:","metadata":{}},{"cell_type":"markdown","source":"### a. Calculating Metrics","metadata":{}},{"cell_type":"code","source":"model.eval()  # Set model to evaluation mode\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in val_loader:  # Use validation DataLoader\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].cpu().numpy()\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        val_preds.extend(probs)\n        val_true.extend(labels)\n\nval_preds = np.array(val_preds)\nval_true = np.array(val_true)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification Report (threshold to be controlled)\nprint(classification_report(\n    val_true, \n    val_preds > 0.5,  # Binary predictions\n    target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Visualizing:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nfor i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    precision, recall, _ = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    plt.plot(recall, precision, label=label)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.legend()\nplt.title(\"Precision-Recall Curves\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Finding Optimal Threshold:","metadata":{}},{"cell_type":"code","source":"optimal_thresholds = []\nfor i in range(6):\n    precision, recall, thresholds = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n    optimal_thresholds.append(thresholds[np.argmax(f1_scores)])\nprint(f\"Optimal Thresholds: {optimal_thresholds}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_summary())  # Place after each epoch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Making Predictions:","metadata":{}},{"cell_type":"code","source":"# 1. Tokenize test data\ntest_encodings = tokenizer(\n    df_test[\"comment_text\"].tolist(),\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Define Dataset class\nclass TestDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n        }\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Create Dataset and Loader\ntest_dataset = TestDataset(test_encodings)\nloader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Run inference\nmodel.eval()\nall_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(loader, desc=\"Processing\"):\n        inputs = {k: v.to(model.device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        all_probs.extend(probs)\n\n        # Memory cleanup\n        del inputs, outputs, batch\n        torch.cuda.empty_cache()\n\n# 5. Final predictions array\nprobs = np.vstack(all_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport numpy as np\n\ndef find_optimal_thresholds(val_true, val_preds, toxicity_classes):\n    \"\"\"\n    Calculate optimal thresholds maximizing F1 for each class\n    \n    Args:\n        val_true: Array of true labels (n_samples × n_classes)\n        val_preds: Array of predicted probabilities (n_samples × n_classes)\n        toxicity_classes: List of class names\n        \n    Returns:\n        Dictionary of {class_name: optimal_threshold}\n    \"\"\"\n    optimal_thresholds = {}\n    \n    for i, class_name in enumerate(toxicity_classes):\n        # Get precision-recall curve for this class\n        precision, recall, thresholds = precision_recall_curve(\n            val_true[:, i], \n            val_preds[:, i]\n        )\n        \n        # Calculating F1 scores\n        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n        \n        # Finding threshold with max F1\n        optimal_idx = np.argmax(f1_scores)\n        optimal_threshold = thresholds[optimal_idx]\n        \n        # Store optimal threshold in dictionary\n        optimal_thresholds[class_name] = optimal_threshold\n    \n    return optimal_thresholds\n\n# Calling the function\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\noptimal_thresholds = find_optimal_thresholds(val_true, val_preds, toxicity_classes)\n\n# Resulting dictionary\nprint(optimal_thresholds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimal thresholds (tune these on your validation set)\nclass_thresholds = optimal_thresholds\n\n# Add probability columns (unchanged)\nfor i, col in enumerate(toxicity_classes):\n    df_test[f\"{col}_prob\"] = probs[:, i]\n    \n# Apply custom thresholds for binary predictions\nfor col in toxicity_classes:\n    df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)\n\n# Check class distribution after thresholding\nprint(\"\\nPredicted Positive Rates:\")\nfor col in toxicity_classes:\n    rate = df_test[f\"{col}_pred\"].mean()\n    print(f\"{col}: {rate:.2%} (threshold={class_thresholds[col]})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs[:, i]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = df_test.drop(['comment_text'], axis = 1)\nfor col in toxicity_classes:\n    submission.drop([f\"{col}_prob\"], inplace = True, axis = 1)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('bert_model_weights')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:09:53.067869Z","iopub.execute_input":"2025-04-04T19:09:53.068234Z","iopub.status.idle":"2025-04-04T19:09:53.816398Z","shell.execute_reply.started":"2025-04-04T19:09:53.068207Z","shell.execute_reply":"2025-04-04T19:09:53.815566Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 99. Try","metadata":{}},{"cell_type":"markdown","source":"#### Single Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"# Load your trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\nmodel.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Move model to device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n\n# Your input sentence\nsentence = \"SLUT\"\n\n# Tokenize and predict\ninputs = tokenizer(sentence, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # Get probabilities\n\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n\n# results\nprint(sentence)\n\nprint(\"Predicted probabilities:\")\nfor cls, prob in zip(toxicity_classes, probs):\n    print(f\"{cls}: {prob:.4f}\")\n\n# binary predictions\nbinary_preds = (probs > 0.5).astype(int)\nprint(\"\\nBinary predictions (threshold=0.5):\")\nfor cls, pred in zip(toxicity_classes, binary_preds):\n    print(f\"{cls}: {'✅' if pred else '❌'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:10:27.987520Z","iopub.execute_input":"2025-04-04T19:10:27.987929Z","iopub.status.idle":"2025-04-04T19:10:29.213953Z","shell.execute_reply.started":"2025-04-04T19:10:27.987894Z","shell.execute_reply":"2025-04-04T19:10:29.212933Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-48-f9cb5ea84647>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\n","output_type":"stream"},{"name":"stdout","text":"SLUT\nPredicted probabilities:\ntoxic: 0.9474\nsevere_toxic: 0.0565\nobscene: 0.7306\nthreat: 0.0058\ninsult: 0.6499\nidentity_hate: 0.2418\n\nBinary predictions (threshold=0.5):\ntoxic: ✅\nsevere_toxic: ❌\nobscene: ✅\nthreat: ❌\ninsult: ✅\nidentity_hate: ❌\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"#### Multi-Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"sentences = [\n    \"You're stupid!\",\n    \"Thanks for your help\",\n    \"Go back to your country\"\n]\n\n# Tokenize batch\ninputs = tokenizer(sentences, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\n# Predict\nwith torch.no_grad():\n    outputs = model(**inputs)\n    all_probs = torch.sigmoid(outputs.logits).cpu().numpy()\n\n# Display results\nfor i, sentence in enumerate(sentences):\n    print(f\"\\nSentence: '{sentence}'\")\n    for cls, prob in zip(toxicity_classes, all_probs[i]):\n        print(f\"{cls}: {prob:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Threat Problem:","metadata":{}},{"cell_type":"markdown","source":"### a. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes=6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.ce_loss = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, inputs, targets):\n        loss = self.ce_loss(inputs, targets)\n        p_t = torch.exp(-loss)\n        focal_loss = self.alpha * (1 - p_t) ** self.gamma * loss\n        return focal_loss.mean()\n\nclass CustomBERTForSequenceClassificationWithFocalLoss(BertForSequenceClassification):\n    def __init__(self, config, focal_loss_alpha=0.25, focal_loss_gamma=2):\n        super().__init__(config)\n        self.focal_loss = FocalLoss(alpha=focal_loss_alpha, gamma=focal_loss_gamma)\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        # Call the parent model's forward method\n        outputs = super().forward(input_ids=input_ids, \n                                  attention_mask=attention_mask, \n                                  token_type_ids=token_type_ids, \n                                  **kwargs)\n        logits = outputs.logits\n\n        # Compute loss if labels are provided\n        if labels is not None:\n            loss = self.focal_loss(logits, labels)\n            return (loss, outputs)\n        else:\n            return outputs\n\n# Example usage:\nmodel = CustomBERTForSequenceClassificationWithFocalLoss.from_pretrained(\"bert-base-uncased\", num_labels=6).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Class Weights","metadata":{}},{"cell_type":"code","source":"'''\n# Calculate class weights (inverse of class frequencies)\nclass_counts = np.array([sum(train_labels[:, i]) for i in range(6)])  # Count per class\nclass_weights = torch.tensor(\n    (1.0 / (class_counts + 1e-6)) * (len(train_labels)/6),  # Normalize\n    dtype=torch.float32,\n    device=device\n)\n\n# Modify your model initialization\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n)\nmodel.loss_fct = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1-pt)**self.gamma * bce_loss).mean()\n        return loss\n\nmodel.loss_fct = FocalLoss(gamma=2).to(device)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### d. Data Loader","metadata":{}},{"cell_type":"code","source":"'''\nfrom torch.utils.data import WeightedRandomSampler\n\n# Calculate sample weights (higher for threat-containing samples)\nsample_weights = torch.where(\n    train_labels[:, 3] == 1,  # Threat is index 3\n    torch.tensor(50.0),       # 50x higher sampling for threats\n    torch.tensor(1.0)\n)\n\nsampler = WeightedRandomSampler(\n    sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Modify your DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    sampler=sampler,  # Replaces shuffle=True\n    num_workers=4\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}