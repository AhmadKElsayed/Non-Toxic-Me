{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:19:25.854839Z","iopub.execute_input":"2025-04-05T10:19:25.855155Z","iopub.status.idle":"2025-04-05T10:19:25.861721Z","shell.execute_reply.started":"2025-04-05T10:19:25.855130Z","shell.execute_reply":"2025-04-05T10:19:25.860783Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 0. Importing Libraries","metadata":{}},{"cell_type":"code","source":"#Main\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#Utilities\nimport random\nimport re\nimport pickle\nfrom tqdm.auto import tqdm\n\n\n#Augmentation\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('punkt')\nfrom nltk.corpus import wordnet\n\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, precision_recall_curve\n\n\n#torch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#Hugging Face\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers import set_seed\nfrom transformers import get_linear_schedule_with_warmup\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Random States:","metadata":{}},{"cell_type":"code","source":"# Python and Numpy\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Save states (optional, for later restoration)\npython_state = random.getstate()\nnumpy_state = np.random.get_state()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch \ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # For multi-GPU\ntorch.backends.cudnn.deterministic = True  # Slower but reproducible\ntorch.backends.cudnn.benchmark = False\n\n# Save RNG states\ntorch_rng_state = torch.get_rng_state()\ncuda_rng_state = torch.cuda.get_rng_state() if torch.cuda.is_available() else None","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hugging Face\nset_seed(seed)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving all random states\n\nrandom_states = {\n    \"python\": random.getstate(),\n    \"numpy\": np.random.get_state(),\n    \"torch_cpu\": torch.get_rng_state(),\n    \"torch_cuda\": torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n    \"sklearn_seed\": seed  # For train_test_split\n}\n\n# Save to file\nwith open(\"random_states.pkl\", \"wb\") as f:\n    pickle.dump(random_states, f)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Load and Preprocess the Data:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ndf_test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[df_train['toxic'] == 1, ['comment_text']].iloc[0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to remove punctuation using regular expressions\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_punctuation)\n\n# Define a function to remove special characters using regular expressions\ndef remove_special_characters(text):\n    # Define a regular expression pattern to match special characters\n    pattern = r'[^a-zA-Z0-9\\s]'  # This pattern matches any character that is not a letter, digit, or whitespace\n    return re.sub(pattern, '', text)\n\n# Apply the function to the 'text' column\ndf_train['comment_text'] = df_train['comment_text'].apply(remove_special_characters)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\nlabel_counts = df_train[label_cols].sum()\nmax_class_count = label_counts.max()\n\nprint(\"Original class distribution:\\n\", label_counts)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_nltk():\n    \"\"\"Handle NLTK data download and path configuration\"\"\"\n    try:\n        nltk_data_dir = '/kaggle/working/nltk_data'\n        os.makedirs(nltk_data_dir, exist_ok=True)\n        \n        if not nltk.data.find('corpora/wordnet'):\n            nltk.download('wordnet', download_dir=nltk_data_dir)\n            nltk.download('omw-1.4', download_dir=nltk_data_dir)\n        \n        nltk.data.path.append(nltk_data_dir)\n        wordnet.ensure_loaded()\n        return True\n    except Exception as e:\n        print(f\"Failed to setup NLTK: {str(e)}\")\n        return False\n\ndef safe_synonym_replacement(text, n=2):\n    \"\"\"Robust synonym replacement with full error handling\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return text\n        \n    try:\n        words = text.split()\n        if len(words) == 0:\n            return text\n            \n        replaceable_words = []\n        for word in words:\n            try:\n                if wordnet.synsets(word):\n                    replaceable_words.append(word)\n            except:\n                continue\n                \n        if not replaceable_words:\n            return text\n            \n        words_to_replace = random.sample(replaceable_words, min(n, len(replaceable_words)))\n        \n        for word in words_to_replace:\n            try:\n                synonyms = []\n                for syn in wordnet.synsets(word):\n                    for lemma in syn.lemmas():\n                        lemma_name = lemma.name().replace('_', ' ')\n                        if lemma_name.lower() != word.lower():\n                            synonyms.append(lemma_name)\n                if synonyms:\n                    synonym = random.choice(list(set(synonyms)))\n                    text = text.replace(word, synonym, 1)\n            except:\n                continue\n                \n        return text\n    except Exception as e:\n        print(f\"Error in synonym replacement: {str(e)}\")\n        return text\n\ndef balance_dataset(df, label_cols):\n    \"\"\"Main augmentation function with complete error handling\"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n        \n    missing_cols = [col for col in ['comment_text'] + label_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n    \n    try:\n        label_counts = df[label_cols].sum()\n        max_count = label_counts.max()\n        print(\"Original distribution:\\n\", label_counts)\n        \n        augmented = []\n        for label in label_cols:\n            needed = max(0, int(0.5 * (max_count - label_counts[label])))\n            if needed <= 0:\n                continue\n                \n            samples = df[df[label] == 1]\n            if len(samples) == 0:\n                continue\n                \n            print(f\"Augmenting {label} (+{needed})\")\n            \n            for _ in range(needed):\n                try:\n                    sample = samples.sample(1).iloc[0]\n                    new_sample = sample.copy()\n                    new_sample['comment_text'] = safe_synonym_replacement(sample['comment_text'])\n                    augmented.append(new_sample)\n                except Exception as e:\n                    print(f\"Skipping sample due to error: {str(e)}\")\n                    continue\n                    \n        if augmented:\n            return pd.concat([df, pd.DataFrame(augmented)], ignore_index=True)\n        return df.copy()\n    except Exception as e:\n        print(f\"Fatal error in balancing: {str(e)}\")\n        return df.copy()\n\n# ===== MAIN EXECUTION =====\nif not setup_nltk():\n    print(\"Warning: Proceeding without WordNet - augmentation will be limited\")\n\ntry:\n    # Load your data (replace with your actual loading code)\n    # df_train = pd.read_csv('your_data.csv')\n    \n    # Define your target labels\n    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    \n    # Run augmentation\n    df_balanced = balance_dataset(df_train, label_cols)\n    \n    # Verify results\n    print(\"\\nNew distribution:\")\n    print(df_balanced[label_cols].sum())\n    \n    # Save if needed\n    # df_balanced.to_csv('balanced_data.csv', index=False)\n    \nexcept Exception as e:\n    print(f\"Fatal error in main execution: {str(e)}\")\n    # Fallback to original data if complete failure\n    df_balanced = df_train.copy()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nlabel_counts = df_train[label_cols].sum()\nmax_class_count = label_counts.max()\n\nprint(\"Original class distribution:\\n\", label_counts)\n\nlabel_counts = df_balanced[label_cols].sum()\nmax_class_count = label_counts.max()\n\nprint(\"Augmented class distribution:\\n\", label_counts)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['comment_text'].duplicated().sum()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[df_train['comment_text'].duplicated()]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_duplicates_with_word(df, word, label=None, show_samples=5):\n    \"\"\"\n    Find duplicate comments containing specific words, optionally filtered by label\n    \n    Parameters:\n    - df: Your DataFrame\n    - word: Word/phrase to search for\n    - label: Optional specific label to check (e.g., 'threat')\n    - show_samples: Number of examples to display\n    \"\"\"\n    # Get all duplicates\n    duplicates = df[df['comment_text'].duplicated(keep=False)]\n    \n    # Filter for word\n    mask = duplicates['comment_text'].str.contains(word, case=False, regex=False)\n    \n    # Optional label filter\n    if label:\n        mask &= (duplicates[label] == 1)\n    \n    results = duplicates[mask]\n    \n    # Display findings\n    print(f\"\\nFound {len(results)} duplicates containing '{word}'\", \n          f\"(with {label}=1)\" if label else \"\")\n    \n    if not results.empty:\n        display(results[['comment_text'] + label_cols].head(show_samples))\n    \n    return results\n\n# Example usage:\nthreat_dupes = find_duplicates_with_word(df_train, \"\\n\\n A barnstar for you \\n\\n The Original Bar\", label='threat')\ntoxic_dupes = find_duplicates_with_word(df_train, \"barnstar\", label='toxic')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.drop_duplicates(inplace = True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_keep = df_balanced.copy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\nlabel_counts = df_train[label_cols].sum()\nmax_class_count = label_counts.max()\n\nprint(\"Original class distribution:\\n\", label_counts)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"comments = df_train[\"comment_text\"].tolist()\nlabels = df_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n\ntrain_comments, val_comments, train_labels, val_labels = train_test_split(\n    comments[:50], labels[:50], test_size=0.2, random_state=seed\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Tokenization:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Changed to BERT\n\ndef tokenize(texts):\n    return tokenizer(\n        texts, \n        padding=True, \n        truncation=True, \n        max_length=128, \n        return_tensors=\"pt\"\n    )\n\n# Tokenize data (works exactly the same way)\ntrain_encodings = tokenize(train_comments)\nval_encodings = tokenize(val_comments)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. PyTorch Dataset:","metadata":{}},{"cell_type":"code","source":"class ToxicDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": torch.FloatTensor(self.labels[idx])\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = ToxicDataset(train_encodings, train_labels)\nval_dataset = ToxicDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. BERT Model:","metadata":{}},{"cell_type":"markdown","source":"### a. Load Pre-trained Roberta Model:","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#Ems7ha lw 3awz trg3 el adeem\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1 - pt) ** self.gamma * bce_loss).mean()\n        return loss\n\n# Set Focal Loss as the model's loss function\nmodel.loss_fct = FocalLoss(gamma=2).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Training Loop","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nepochs = 10 # Increased to allow early stopping to work\nlr = 1e-3\nwarmup_steps = 100\nmax_grad_norm = 1.0\npatience = 3  # Number of epochs to wait before stopping\n\n# Training Setup\noptimizer = AdamW(model.parameters(), lr=lr)\n\n# Learning rate schedule\ndef lr_lambda(current_step):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    return 1.0\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n# Tracking\nbest_metrics = {\n    'val_loss': float('inf'),\n    'weights': None,\n    'epoch': -1\n}\nhistory = []\nepochs_without_improvement = 0  # Early stopping counter\n\nfor epoch in range(epochs):\n    # --- Training Phase ---\n    model.train()\n    train_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(model.device)\n        \n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n    \n    avg_train_loss = train_loss / len(train_loader)\n\n    # --- Validation Phase ---\n    avg_val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].to(model.device)\n            \n            outputs = model(**inputs, labels=labels)\n            avg_val_loss += outputs.loss.item()\n    \n    avg_val_loss /= len(val_loader)\n\n    # --- Early Stopping Check ---\n    if avg_val_loss < best_metrics['val_loss']:\n        best_metrics.update({\n            'val_loss': avg_val_loss,\n            'weights': model.state_dict().copy(),\n            'epoch': epoch + 1\n        })\n        torch.save(best_metrics['weights'], \"best_model.pt\")\n        epochs_without_improvement = 0  # Reset counter\n        print(f\"↳ New best model saved! (Loss: {avg_val_loss:.4f})\")\n    else:\n        epochs_without_improvement += 1\n        print(f\"↳ No improvement ({epochs_without_improvement}/{patience})\")\n        \n        if epochs_without_improvement >= patience:\n            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}!\")\n            print(f\"Best model was from epoch {best_metrics['epoch']} with val_loss {best_metrics['val_loss']:.4f}\")\n            break\n\n    # --- Progress Tracking ---\n    history.append({\n        'epoch': epoch + 1,\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'early_stop_counter': epochs_without_improvement\n    })\n    \n    print(f\"\\nEpoch {epoch + 1} Results:\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n# Final save if no best model was found\nif not os.path.exists(\"best_model.pt\"):\n    torch.save(model.state_dict(), \"final_model.pt\")\n    print(\"Saved final model weights (no improvement during training)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Model Evaluation:","metadata":{}},{"cell_type":"markdown","source":"### a. Calculating Metrics","metadata":{}},{"cell_type":"code","source":"model.eval()  # Set model to evaluation mode\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in val_loader:  # Use validation DataLoader\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].cpu().numpy()\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        val_preds.extend(probs)\n        val_true.extend(labels)\n\nval_preds = np.array(val_preds)\nval_true = np.array(val_true)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification Report (threshold to be controlled)\nprint(classification_report(\n    val_true, \n    val_preds > 0.5,  # Binary predictions\n    target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Visualizing:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nfor i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    precision, recall, _ = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    plt.plot(recall, precision, label=label)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.legend()\nplt.title(\"Precision-Recall Curves\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Finding Optimal Threshold:","metadata":{}},{"cell_type":"code","source":"optimal_thresholds = []\nfor i in range(6):\n    precision, recall, thresholds = precision_recall_curve(val_true[:, i], val_preds[:, i])\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n    optimal_thresholds.append(thresholds[np.argmax(f1_scores)])\nprint(f\"Optimal Thresholds: {optimal_thresholds}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Making Predictions:","metadata":{}},{"cell_type":"code","source":"# 1. Tokenize test data\ntest_encodings = tokenizer(\n    df_test[\"comment_text\"].tolist(),\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Define Dataset class\nclass TestDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n        }\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Create Dataset and Loader\ntest_dataset = TestDataset(test_encodings)\nloader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Run inference\nmodel.eval()\nall_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(loader, desc=\"Processing\"):\n        inputs = {k: v.to(model.device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n        all_probs.extend(probs)\n\n        # Memory cleanup\n        del inputs, outputs, batch\n        torch.cuda.empty_cache()\n\n# 5. Final predictions array\nprobs = np.vstack(all_probs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_optimal_thresholds(val_true, val_preds, toxicity_classes):\n    \"\"\"\n    Calculate optimal thresholds maximizing F1 for each class\n    \n    Args:\n        val_true: Array of true labels (n_samples × n_classes)\n        val_preds: Array of predicted probabilities (n_samples × n_classes)\n        toxicity_classes: List of class names\n        \n    Returns:\n        Dictionary of {class_name: optimal_threshold}\n    \"\"\"\n    optimal_thresholds = {}\n    \n    for i, class_name in enumerate(toxicity_classes):\n        # Get precision-recall curve for this class\n        precision, recall, thresholds = precision_recall_curve(\n            val_true[:, i], \n            val_preds[:, i]\n        )\n        \n        # Calculating F1 scores\n        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n        \n        # Finding threshold with max F1\n        optimal_idx = np.argmax(f1_scores)\n        optimal_threshold = thresholds[optimal_idx]\n        \n        # Store optimal threshold in dictionary\n        optimal_thresholds[class_name] = optimal_threshold\n    \n    return optimal_thresholds\n\n# Calling the function\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\noptimal_thresholds = find_optimal_thresholds(val_true, val_preds, toxicity_classes)\n\n# Resulting dictionary\nprint(optimal_thresholds)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimal thresholds (tune these on your validation set)\nclass_thresholds = optimal_thresholds\n\n# Add probability columns (unchanged)\nfor i, col in enumerate(toxicity_classes):\n    df_test[f\"{col}_prob\"] = probs[:, i]\n    \n# Apply custom thresholds for binary predictions\nfor col in toxicity_classes:\n    df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)\n\n# Check class distribution after thresholding\nprint(\"\\nPredicted Positive Rates:\")\nfor col in toxicity_classes:\n    rate = df_test[f\"{col}_pred\"].mean()\n    print(f\"{col}: {rate:.2%} (threshold={class_thresholds[col]})\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs[:, i]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = df_test.drop(['comment_text'], axis = 1)\nfor col in toxicity_classes:\n    submission.drop([f\"{col}_prob\"], inplace = True, axis = 1)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('bert_model_weights')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 99. Try","metadata":{}},{"cell_type":"markdown","source":"#### Single Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"'''\n# Load your trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\nmodel.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n'''\n\n# Move model to device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n\n# Your input sentence\nsentence = \"SLUT\"\n\n# Tokenize and predict\ninputs = tokenizer(sentence, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # Get probabilities\n\ntoxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n\n# results\nprint(sentence)\n\nprint(\"Predicted probabilities:\")\nfor cls, prob in zip(toxicity_classes, probs):\n    print(f\"{cls}: {prob:.4f}\")\n\n# binary predictions\nbinary_preds = (probs > 0.5).astype(int)\nprint(\"\\nBinary predictions (threshold=0.5):\")\nfor cls, pred in zip(toxicity_classes, binary_preds):\n    print(f\"{cls}: {'✅' if pred else '❌'}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Multi-Sentence Prediction:","metadata":{}},{"cell_type":"code","source":"sentences = [\n    \"You're stupid!\",\n    \"Thanks for your help\",\n    \"Go back to your country\"\n]\n\n# Tokenize batch\ninputs = tokenizer(sentences, \n                 padding=True, \n                 truncation=True, \n                 max_length=128, \n                 return_tensors=\"pt\").to(device)\n\n# Predict\nwith torch.no_grad():\n    outputs = model(**inputs)\n    all_probs = torch.sigmoid(outputs.logits).cpu().numpy()\n\n# Display results\nfor i, sentence in enumerate(sentences):\n    print(f\"\\nSentence: '{sentence}'\")\n    for cls, prob in zip(toxicity_classes, all_probs[i]):\n        print(f\"{cls}: {prob:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Threat Problem:","metadata":{}},{"cell_type":"markdown","source":"### a. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes=6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.ce_loss = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, inputs, targets):\n        loss = self.ce_loss(inputs, targets)\n        p_t = torch.exp(-loss)\n        focal_loss = self.alpha * (1 - p_t) ** self.gamma * loss\n        return focal_loss.mean()\n\nclass CustomBERTForSequenceClassificationWithFocalLoss(BertForSequenceClassification):\n    def __init__(self, config, focal_loss_alpha=0.25, focal_loss_gamma=2):\n        super().__init__(config)\n        self.focal_loss = FocalLoss(alpha=focal_loss_alpha, gamma=focal_loss_gamma)\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        # Call the parent model's forward method\n        outputs = super().forward(input_ids=input_ids, \n                                  attention_mask=attention_mask, \n                                  token_type_ids=token_type_ids, \n                                  **kwargs)\n        logits = outputs.logits\n\n        # Compute loss if labels are provided\n        if labels is not None:\n            loss = self.focal_loss(logits, labels)\n            return (loss, outputs)\n        else:\n            return outputs\n\n# Example usage:\nmodel = CustomBERTForSequenceClassificationWithFocalLoss.from_pretrained(\"bert-base-uncased\", num_labels=6).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b. Class Weights","metadata":{}},{"cell_type":"code","source":"'''\n# Calculate class weights (inverse of class frequencies)\nclass_counts = np.array([sum(train_labels[:, i]) for i in range(6)])  # Count per class\nclass_weights = torch.tensor(\n    (1.0 / (class_counts + 1e-6)) * (len(train_labels)/6),  # Normalize\n    dtype=torch.float32,\n    device=device\n)\n\n# Modify your model initialization\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=6,\n    problem_type=\"multi_label_classification\"\n)\nmodel.loss_fct = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c. Focal Loss","metadata":{}},{"cell_type":"code","source":"'''\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        loss = ((1-pt)**self.gamma * bce_loss).mean()\n        return loss\n\nmodel.loss_fct = FocalLoss(gamma=2).to(device)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### d. Data Loader","metadata":{}},{"cell_type":"code","source":"'''\nfrom torch.utils.data import WeightedRandomSampler\n\n# Calculate sample weights (higher for threat-containing samples)\nsample_weights = torch.where(\n    train_labels[:, 3] == 1,  # Threat is index 3\n    torch.tensor(50.0),       # 50x higher sampling for threats\n    torch.tensor(1.0)\n)\n\nsampler = WeightedRandomSampler(\n    sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Modify your DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    sampler=sampler,  # Replaces shuffle=True\n    num_workers=4\n)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augmentation\n'''\n# Your settings\nTARGET_LABELS = {\n    'severe_toxic': 5000,\n    'threat': 10000, \n    'identity_hate': 8000\n}\n\n# Track existing texts to prevent duplicates\nexisting_texts = set(df_train['comment_text'].tolist())\n\ndef augment_text(text, n=5):\n    \"\"\"Your original augmentation function with duplicate check\"\"\"\n    original_text = text\n    for _ in range(n):\n        try:\n            words = text.split()\n            replaceable = [w for w in words if wordnet.synsets(w)]\n            if not replaceable:\n                break\n                \n            word = random.choice(replaceable)\n            synonyms = wordnet.synsets(word)\n            if synonyms:\n                lemma = random.choice([l for s in synonyms for l in s.lemmas()])\n                new_text = text.replace(word, lemma.name().replace('_', ' '), 1)\n                if new_text != original_text:  # Only keep meaningful changes\n                    text = new_text\n        except:\n            continue\n    return text if text != original_text else None  # Return None if no changes made\n\n# Main augmentation\naugmented = []\nfor label, count in tqdm(TARGET_LABELS.items(), desc=\"Augmenting\"):\n    samples = df_train[df_train[label] == 1]\n    if len(samples) == 0:\n        continue\n        \n    with tqdm(total=count, desc=f\"{label}\") as pbar:\n        generated = 0\n        while generated < count:\n            original = samples.sample(1).iloc[0]\n            new_text = augment_text(original['comment_text'])\n            \n            if new_text and new_text not in existing_texts:\n                new_row = original.copy()\n                new_row['comment_text'] = new_text\n                augmented.append(new_row)\n                existing_texts.add(new_text)\n                generated += 1\n                pbar.update(1)\n\n# Final deduplication (just in case)\naugmented = [dict(t) for t in {tuple(d.items()) for d in augmented}]\n\n# Combine results\nif augmented:\n    df_train = pd.concat([\n        df_train,\n        pd.DataFrame(augmented)\n    ], ignore_index=True)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T10:20:27.239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}