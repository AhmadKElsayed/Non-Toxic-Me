{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Main\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Utilities\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "#Augmentation\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_curve\n",
    "\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Hugging Face\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random States:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Python and Numpy\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Save states (optional, for later restoration)\n",
    "python_state = random.getstate()\n",
    "numpy_state = np.random.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "torch.backends.cudnn.deterministic = True  # Slower but reproducible\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Save RNG states\n",
    "torch_rng_state = torch.get_rng_state()\n",
    "cuda_rng_state = torch.cuda.get_rng_state() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hugging Face\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Saving all random states\n",
    "\n",
    "random_states = {\n",
    "    \"python\": random.getstate(),\n",
    "    \"numpy\": np.random.get_state(),\n",
    "    \"torch_cpu\": torch.get_rng_state(),\n",
    "    \"torch_cuda\": torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n",
    "    \"sklearn_seed\": seed  # For train_test_split\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"random_states.pkl\", \"wb\") as f:\n",
    "    pickle.dump(random_states, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Preprocess the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "df_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n",
    "df_test_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.loc[df_train['toxic'] == 1, ['comment_text']].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a function to remove punctuation using regular expressions\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df_train['comment_text'] = df_train['comment_text'].apply(remove_punctuation)\n",
    "\n",
    "# Define a function to remove special characters using regular expressions\n",
    "def remove_special_characters(text):\n",
    "    # Define a regular expression pattern to match special characters\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'  # This pattern matches any character that is not a letter, digit, or whitespace\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df_train['comment_text'] = df_train['comment_text'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "label_counts = df_train[label_cols].sum()\n",
    "max_class_count = label_counts.max()\n",
    "\n",
    "print(\"Original class distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_nltk():\n",
    "    \"\"\"Handle NLTK data download and path configuration\"\"\"\n",
    "    try:\n",
    "        nltk_data_dir = '/kaggle/working/nltk_data'\n",
    "        os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "        \n",
    "        if not nltk.data.find('corpora/wordnet'):\n",
    "            nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "            nltk.download('omw-1.4', download_dir=nltk_data_dir)\n",
    "        \n",
    "        nltk.data.path.append(nltk_data_dir)\n",
    "        wordnet.ensure_loaded()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to setup NLTK: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def safe_synonym_replacement(text, n=2):\n",
    "    \"\"\"Robust synonym replacement with full error handling\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "        \n",
    "    try:\n",
    "        words = text.split()\n",
    "        if len(words) == 0:\n",
    "            return text\n",
    "            \n",
    "        replaceable_words = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                if wordnet.synsets(word):\n",
    "                    replaceable_words.append(word)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if not replaceable_words:\n",
    "            return text\n",
    "            \n",
    "        words_to_replace = random.sample(replaceable_words, min(n, len(replaceable_words)))\n",
    "        \n",
    "        for word in words_to_replace:\n",
    "            try:\n",
    "                synonyms = []\n",
    "                for syn in wordnet.synsets(word):\n",
    "                    for lemma in syn.lemmas():\n",
    "                        lemma_name = lemma.name().replace('_', ' ')\n",
    "                        if lemma_name.lower() != word.lower():\n",
    "                            synonyms.append(lemma_name)\n",
    "                if synonyms:\n",
    "                    synonym = random.choice(list(set(synonyms)))\n",
    "                    text = text.replace(word, synonym, 1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in synonym replacement: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "def balance_dataset(df, label_cols):\n",
    "    \"\"\"Main augmentation function with complete error handling\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "        \n",
    "    missing_cols = [col for col in ['comment_text'] + label_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    try:\n",
    "        label_counts = df[label_cols].sum()\n",
    "        max_count = label_counts.max()\n",
    "        print(\"Original distribution:\\n\", label_counts)\n",
    "        \n",
    "        augmented = []\n",
    "        for label in label_cols:\n",
    "            needed = max(0, int(0.5 * (max_count - label_counts[label])))\n",
    "            if needed <= 0:\n",
    "                continue\n",
    "                \n",
    "            samples = df[df[label] == 1]\n",
    "            if len(samples) == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Augmenting {label} (+{needed})\")\n",
    "            \n",
    "            for _ in range(needed):\n",
    "                try:\n",
    "                    sample = samples.sample(1).iloc[0]\n",
    "                    new_sample = sample.copy()\n",
    "                    new_sample['comment_text'] = safe_synonym_replacement(sample['comment_text'])\n",
    "                    augmented.append(new_sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping sample due to error: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        if augmented:\n",
    "            return pd.concat([df, pd.DataFrame(augmented)], ignore_index=True)\n",
    "        return df.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error in balancing: {str(e)}\")\n",
    "        return df.copy()\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if not setup_nltk():\n",
    "    print(\"Warning: Proceeding without WordNet - augmentation will be limited\")\n",
    "\n",
    "try:\n",
    "    # Load your data (replace with your actual loading code)\n",
    "    # df_train = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Define your target labels\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # Run augmentation\n",
    "    df_balanced = balance_dataset(df_train, label_cols)\n",
    "    \n",
    "    # Verify results\n",
    "    print(\"\\nNew distribution:\")\n",
    "    print(df_balanced[label_cols].sum())\n",
    "    \n",
    "    # Save if needed\n",
    "    # df_balanced.to_csv('balanced_data.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error in main execution: {str(e)}\")\n",
    "    # Fallback to original data if complete failure\n",
    "    df_balanced = df_train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "label_counts = df_train[label_cols].sum()\n",
    "max_class_count = label_counts.max()\n",
    "\n",
    "print(\"Original class distribution:\\n\", label_counts)\n",
    "\n",
    "label_counts = df_balanced[label_cols].sum()\n",
    "max_class_count = label_counts.max()\n",
    "\n",
    "print(\"Augmented class distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['comment_text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_duplicates_with_word(df, word, label=None, show_samples=5):\n",
    "    \"\"\"\n",
    "    Find duplicate comments containing specific words, optionally filtered by label\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Your DataFrame\n",
    "    - word: Word/phrase to search for\n",
    "    - label: Optional specific label to check (e.g., 'threat')\n",
    "    - show_samples: Number of examples to display\n",
    "    \"\"\"\n",
    "    # Get all duplicates\n",
    "    duplicates = df[df['comment_text'].duplicated(keep=False)]\n",
    "    \n",
    "    # Filter for word\n",
    "    mask = duplicates['comment_text'].str.contains(word, case=False, regex=False)\n",
    "    \n",
    "    # Optional label filter\n",
    "    if label:\n",
    "        mask &= (duplicates[label] == 1)\n",
    "    \n",
    "    results = duplicates[mask]\n",
    "    \n",
    "    # Display findings\n",
    "    print(f\"\\nFound {len(results)} duplicates containing '{word}'\", \n",
    "          f\"(with {label}=1)\" if label else \"\")\n",
    "    \n",
    "    if not results.empty:\n",
    "        display(results[['comment_text'] + label_cols].head(show_samples))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "threat_dupes = find_duplicates_with_word(df_train, \"\\n\\n A barnstar for you \\n\\n The Original Bar\", label='threat')\n",
    "toxic_dupes = find_duplicates_with_word(df_train, \"barnstar\", label='toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "label_counts = df_train[label_cols].sum()\n",
    "max_class_count = label_counts.max()\n",
    "\n",
    "print(\"Original class distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comments = df_balanced[\"comment_text\"].tolist()\n",
    "labels = df_balanced[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "\n",
    "train_comments, val_comments, train_labels, val_labels = train_test_split(\n",
    "    comments, labels, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "#emsa7 el [:50] de lma t3ooz trun lkloh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Changed to BERT\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenize data (works exactly the same way)\n",
    "train_encodings = tokenize(train_comments)\n",
    "val_encodings = tokenize(val_comments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PyTorch Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": torch.FloatTensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ToxicDataset(train_encodings, train_labels)\n",
    "val_dataset = ToxicDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERT Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load Pre-trained Roberta Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Ems7ha lw 3awz trg3 el adeem\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = ((1 - pt) ** self.gamma * bce_loss).mean()\n",
    "        return loss\n",
    "\n",
    "# Set Focal Loss as the model's loss function\n",
    "model.loss_fct = FocalLoss(gamma=2).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 10 # Increased to allow early stopping to work\n",
    "lr = 2e-5\n",
    "warmup_steps = 100\n",
    "max_grad_norm = 1.0\n",
    "patience = 3  # Number of epochs to wait before stopping\n",
    "\n",
    "# Training Setup\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Tracking\n",
    "best_metrics = {\n",
    "    'val_loss': float('inf'),\n",
    "    'weights': None,\n",
    "    'epoch': -1\n",
    "}\n",
    "history = []\n",
    "epochs_without_improvement = 0  # Early stopping counter\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        \n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    avg_val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            avg_val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss /= len(val_loader)\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if avg_val_loss < best_metrics['val_loss']:\n",
    "        best_metrics.update({\n",
    "            'val_loss': avg_val_loss,\n",
    "            'weights': model.state_dict().copy(),\n",
    "            'epoch': epoch + 1\n",
    "        })\n",
    "        torch.save(best_metrics['weights'], \"best_model.pt\")\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        print(f\"↳ New best model saved! (Loss: {avg_val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"↳ No improvement ({epochs_without_improvement}/{patience})\")\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}!\")\n",
    "            print(f\"Best model was from epoch {best_metrics['epoch']} with val_loss {best_metrics['val_loss']:.4f}\")\n",
    "            break\n",
    "\n",
    "    # --- Progress Tracking ---\n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'early_stop_counter': epochs_without_improvement\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Final save if no best model was found\n",
    "if not os.path.exists(\"best_model.pt\"):\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\"Saved final model weights (no improvement during training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "val_preds = []\n",
    "val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:  # Use validation DataLoader\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        val_preds.extend(probs)\n",
    "        val_true.extend(labels)\n",
    "\n",
    "val_preds = np.array(val_preds)\n",
    "val_true = np.array(val_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Load your trained model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "'''\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Your input sentence\n",
    "sentence = \"GAYS ARE COOL\"\n",
    "\n",
    "# Tokenize and predict\n",
    "inputs = tokenizer(sentence, \n",
    "                 padding=True, \n",
    "                 truncation=True, \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # Get probabilities\n",
    "\n",
    "toxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "\n",
    "# results\n",
    "print(sentence)\n",
    "\n",
    "print(\"Predicted probabilities:\")\n",
    "for cls, prob in zip(toxicity_classes, probs):\n",
    "    print(f\"{cls}: {prob:.4f}\")\n",
    "\n",
    "# binary predictions\n",
    "binary_preds = (probs > 0.5).astype(int)\n",
    "print(\"\\nBinary predictions (threshold=0.5):\")\n",
    "for cls, pred in zip(toxicity_classes, binary_preds):\n",
    "    print(f\"{cls}: {'✅' if pred else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Classification Report (threshold to be controlled)\n",
    "print(classification_report(\n",
    "    val_true, \n",
    "    val_preds > 0.5,  # Binary predictions\n",
    "    target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Visualizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    precision, recall, _ = precision_recall_curve(val_true[:, i], val_preds[:, i])\n",
    "    plt.plot(recall, precision, label=label)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Finding Optimal Threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimal_thresholds = []\n",
    "for i in range(6):\n",
    "    precision, recall, thresholds = precision_recall_curve(val_true[:, i], val_preds[:, i])\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    optimal_thresholds.append(thresholds[np.argmax(f1_scores)])\n",
    "print(f\"Optimal Thresholds: {optimal_thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Making Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Tokenize test data\n",
    "test_encodings = tokenizer(\n",
    "    df_test[\"comment_text\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Define Dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Create Dataset and Loader\n",
    "test_dataset = TestDataset(test_encodings)\n",
    "loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Run inference\n",
    "model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Processing\"):\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "        # Memory cleanup\n",
    "        del inputs, outputs, batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 5. Final predictions array\n",
    "probs = np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_thresholds(val_true, val_preds, toxicity_classes):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds maximizing F1 for each class\n",
    "    \n",
    "    Args:\n",
    "        val_true: Array of true labels (n_samples × n_classes)\n",
    "        val_preds: Array of predicted probabilities (n_samples × n_classes)\n",
    "        toxicity_classes: List of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of {class_name: optimal_threshold}\n",
    "    \"\"\"\n",
    "    optimal_thresholds = {}\n",
    "    \n",
    "    for i, class_name in enumerate(toxicity_classes):\n",
    "        # Get precision-recall curve for this class\n",
    "        precision, recall, thresholds = precision_recall_curve(\n",
    "            val_true[:, i], \n",
    "            val_preds[:, i]\n",
    "        )\n",
    "        \n",
    "        # Calculating F1 scores\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        \n",
    "        # Finding threshold with max F1\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        # Store optimal threshold in dictionary\n",
    "        optimal_thresholds[class_name] = optimal_threshold\n",
    "    \n",
    "    return optimal_thresholds\n",
    "\n",
    "# Calling the function\n",
    "toxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "optimal_thresholds = find_optimal_thresholds(val_true, val_preds, toxicity_classes)\n",
    "\n",
    "# Resulting dictionary\n",
    "print(optimal_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimal thresholds (tune these on your validation set)\n",
    "class_thresholds = optimal_thresholds\n",
    "\n",
    "# Add probability columns (unchanged)\n",
    "for i, col in enumerate(toxicity_classes):\n",
    "    df_test[f\"{col}_prob\"] = probs[:, i]\n",
    "    \n",
    "# Apply custom thresholds for binary predictions\n",
    "for col in toxicity_classes:\n",
    "    df_test[f\"{col}\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)\n",
    "\n",
    "# Check class distribution after thresholding\n",
    "print(\"\\nPredicted Positive Rates:\")\n",
    "for col in toxicity_classes:\n",
    "    rate = df_test[f\"{col}\"].mean()\n",
    "    print(f\"{col}: {rate:.2%} (threshold={class_thresholds[col]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#for 0.5 threshold\n",
    "'''\n",
    "for col in toxicity_classes:\n",
    "    df_test.drop([f\"{col}_prob\"], inplace = True, axis = 1)\n",
    "    df_test.drop([f\"{col}_pred\"], inplace = True, axis = 1)\n",
    "\n",
    "# Add probability columns (unchanged)\n",
    "for i, col in enumerate(toxicity_classes):\n",
    "    df_test[f\"{col}_prob\"] = probs[:, i]\n",
    "    \n",
    "# Apply custom thresholds for binary predictions\n",
    "for col in toxicity_classes:\n",
    "    #df_test[f\"{col}_pred\"] = (df_test[f\"{col}_prob\"] > class_thresholds[col]).astype(int)   #When usibng train test split\n",
    "    df_test[f\"{col}\"] = (df_test[f\"{col}_prob\"] > 0.5).astype(int)\n",
    "\n",
    "# Check class distribution after thresholding\n",
    "print(\"\\nPredicted Positive Rates:\")\n",
    "for col in toxicity_classes:\n",
    "    rate = df_test[f\"{col}\"].mean()\n",
    "    print(f\"{col}: {rate:.2%} (threshold={0.5})\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = df_test.drop(['comment_text'], axis = 1)\n",
    "for col in toxicity_classes:\n",
    "    submission.drop([f\"{col}_prob\"], inplace = True, axis = 1)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_balanced.to_csv('augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Sentence Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Load your trained model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))  # or \"final_model.pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "'''\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Your input sentence\n",
    "sentence = \"SLUT\"\n",
    "\n",
    "# Tokenize and predict\n",
    "inputs = tokenizer(sentence, \n",
    "                 padding=True, \n",
    "                 truncation=True, \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # Get probabilities\n",
    "\n",
    "toxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "\n",
    "# results\n",
    "print(sentence)\n",
    "\n",
    "print(\"Predicted probabilities:\")\n",
    "for cls, prob in zip(toxicity_classes, probs):\n",
    "    print(f\"{cls}: {prob:.4f}\")\n",
    "\n",
    "# binary predictions\n",
    "binary_preds = (probs > 0.5).astype(int)\n",
    "print(\"\\nBinary predictions (threshold=0.5):\")\n",
    "for cls, pred in zip(toxicity_classes, binary_preds):\n",
    "    print(f\"{cls}: {'✅' if pred else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Sentence Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"You're stupid!\",\n",
    "    \"Thanks for your help\",\n",
    "    \"Go back to your country BITC\"\n",
    "]\n",
    "\n",
    "# Tokenize batch\n",
    "inputs = tokenizer(sentences, \n",
    "                 padding=True, \n",
    "                 truncation=True, \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    all_probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "# Display results\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence: '{sentence}'\")\n",
    "    for cls, prob in zip(toxicity_classes, all_probs[i]):\n",
    "        print(f\"{cls}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Threat Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Ems7ha lw 3awz trg3 el adeem\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = ((1 - pt) ** self.gamma * bce_loss).mean()\n",
    "        return loss\n",
    "\n",
    "# Set Focal Loss as the model's loss function\n",
    "model.loss_fct = FocalLoss(gamma=2).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Calculate class weights (inverse of class frequencies)\n",
    "class_counts = np.array([sum(train_labels[:, i]) for i in range(6)])  # Count per class\n",
    "class_weights = torch.tensor(\n",
    "    (1.0 / (class_counts + 1e-6)) * (len(train_labels)/6),  # Normalize\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Modify your model initialization\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.loss_fct = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-05T19:47:01.150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Calculate sample weights (higher for threat-containing samples)\n",
    "sample_weights = torch.where(\n",
    "    train_labels[:, 3] == 1,  # Threat is index 3\n",
    "    torch.tensor(50.0),       # 50x higher sampling for threats\n",
    "    torch.tensor(1.0)\n",
    ")\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Modify your DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=sampler,  # Replaces shuffle=True\n",
    "    num_workers=4\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 44219,
     "sourceId": 8076,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
